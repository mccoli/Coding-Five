{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e669f36f",
   "metadata": {},
   "source": [
    "*Adapted from: https://keras.io/examples/vision/mnist_convnet/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b60e0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6ee9c9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# in keras you can define layers\n",
    "from tensorflow.keras import layers\n",
    "# print(keras.__version__)\n",
    "keras.backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "79ff124d-6638-47bf-8c38-a67f6271d6fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\mccal\\miniconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\mccal\\miniconda3\\lib\\site-packages (from pandas) (1.26.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mccal\\miniconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mccal\\miniconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\mccal\\miniconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mccal\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4223bf8",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d439f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream'\n",
      " 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream'\n",
      " 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream'\n",
      " 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Roach'\n",
      " 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach'\n",
      " 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach'\n",
      " 'Roach' 'Whitefish' 'Whitefish' 'Whitefish' 'Whitefish' 'Whitefish'\n",
      " 'Whitefish' 'Parkki' 'Parkki' 'Parkki' 'Parkki' 'Parkki' 'Parkki'\n",
      " 'Parkki' 'Parkki' 'Parkki' 'Parkki' 'Parkki' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Pike'\n",
      " 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike'\n",
      " 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Smelt' 'Smelt' 'Smelt' 'Smelt'\n",
      " 'Smelt' 'Smelt' 'Smelt' 'Smelt' 'Smelt' 'Smelt' 'Smelt' 'Smelt' 'Smelt'\n",
      " 'Smelt']\n",
      "[[2.42000e+02 2.32000e+01 2.54000e+01 3.00000e+01 1.15200e+01 4.02000e+00]\n",
      " [2.90000e+02 2.40000e+01 2.63000e+01 3.12000e+01 1.24800e+01 4.30560e+00]\n",
      " [3.40000e+02 2.39000e+01 2.65000e+01 3.11000e+01 1.23778e+01 4.69610e+00]\n",
      " [3.63000e+02 2.63000e+01 2.90000e+01 3.35000e+01 1.27300e+01 4.45550e+00]\n",
      " [4.30000e+02 2.65000e+01 2.90000e+01 3.40000e+01 1.24440e+01 5.13400e+00]\n",
      " [4.50000e+02 2.68000e+01 2.97000e+01 3.47000e+01 1.36024e+01 4.92740e+00]\n",
      " [5.00000e+02 2.68000e+01 2.97000e+01 3.45000e+01 1.41795e+01 5.27850e+00]\n",
      " [3.90000e+02 2.76000e+01 3.00000e+01 3.50000e+01 1.26700e+01 4.69000e+00]\n",
      " [4.50000e+02 2.76000e+01 3.00000e+01 3.51000e+01 1.40049e+01 4.84380e+00]\n",
      " [5.00000e+02 2.85000e+01 3.07000e+01 3.62000e+01 1.42266e+01 4.95940e+00]\n",
      " [4.75000e+02 2.84000e+01 3.10000e+01 3.62000e+01 1.42628e+01 5.10420e+00]\n",
      " [5.00000e+02 2.87000e+01 3.10000e+01 3.62000e+01 1.43714e+01 4.81460e+00]\n",
      " [5.00000e+02 2.91000e+01 3.15000e+01 3.64000e+01 1.37592e+01 4.36800e+00]\n",
      " [3.40000e+02 2.95000e+01 3.20000e+01 3.73000e+01 1.39129e+01 5.07280e+00]\n",
      " [6.00000e+02 2.94000e+01 3.20000e+01 3.72000e+01 1.49544e+01 5.17080e+00]\n",
      " [6.00000e+02 2.94000e+01 3.20000e+01 3.72000e+01 1.54380e+01 5.58000e+00]\n",
      " [7.00000e+02 3.04000e+01 3.30000e+01 3.83000e+01 1.48604e+01 5.28540e+00]\n",
      " [7.00000e+02 3.04000e+01 3.30000e+01 3.85000e+01 1.49380e+01 5.19750e+00]\n",
      " [6.10000e+02 3.09000e+01 3.35000e+01 3.86000e+01 1.56330e+01 5.13380e+00]\n",
      " [6.50000e+02 3.10000e+01 3.35000e+01 3.87000e+01 1.44738e+01 5.72760e+00]\n",
      " [5.75000e+02 3.13000e+01 3.40000e+01 3.95000e+01 1.51285e+01 5.56950e+00]\n",
      " [6.85000e+02 3.14000e+01 3.40000e+01 3.92000e+01 1.59936e+01 5.37040e+00]\n",
      " [6.20000e+02 3.15000e+01 3.45000e+01 3.97000e+01 1.55227e+01 5.28010e+00]\n",
      " [6.80000e+02 3.18000e+01 3.50000e+01 4.06000e+01 1.54686e+01 6.13060e+00]\n",
      " [7.00000e+02 3.19000e+01 3.50000e+01 4.05000e+01 1.62405e+01 5.58900e+00]\n",
      " [7.25000e+02 3.18000e+01 3.50000e+01 4.09000e+01 1.63600e+01 6.05320e+00]\n",
      " [7.20000e+02 3.20000e+01 3.50000e+01 4.06000e+01 1.63618e+01 6.09000e+00]\n",
      " [7.14000e+02 3.27000e+01 3.60000e+01 4.15000e+01 1.65170e+01 5.85150e+00]\n",
      " [8.50000e+02 3.28000e+01 3.60000e+01 4.16000e+01 1.68896e+01 6.19840e+00]\n",
      " [1.00000e+03 3.35000e+01 3.70000e+01 4.26000e+01 1.89570e+01 6.60300e+00]\n",
      " [9.20000e+02 3.50000e+01 3.85000e+01 4.41000e+01 1.80369e+01 6.30630e+00]\n",
      " [9.55000e+02 3.50000e+01 3.85000e+01 4.40000e+01 1.80840e+01 6.29200e+00]\n",
      " [9.25000e+02 3.62000e+01 3.95000e+01 4.53000e+01 1.87542e+01 6.74970e+00]\n",
      " [9.75000e+02 3.74000e+01 4.10000e+01 4.59000e+01 1.86354e+01 6.74730e+00]\n",
      " [9.50000e+02 3.80000e+01 4.10000e+01 4.65000e+01 1.76235e+01 6.37050e+00]\n",
      " [4.00000e+01 1.29000e+01 1.41000e+01 1.62000e+01 4.14720e+00 2.26800e+00]\n",
      " [6.90000e+01 1.65000e+01 1.82000e+01 2.03000e+01 5.29830e+00 2.82170e+00]\n",
      " [7.80000e+01 1.75000e+01 1.88000e+01 2.12000e+01 5.57560e+00 2.90440e+00]\n",
      " [8.70000e+01 1.82000e+01 1.98000e+01 2.22000e+01 5.61660e+00 3.17460e+00]\n",
      " [1.20000e+02 1.86000e+01 2.00000e+01 2.22000e+01 6.21600e+00 3.57420e+00]\n",
      " [0.00000e+00 1.90000e+01 2.05000e+01 2.28000e+01 6.47520e+00 3.35160e+00]\n",
      " [1.10000e+02 1.91000e+01 2.08000e+01 2.31000e+01 6.16770e+00 3.39570e+00]\n",
      " [1.20000e+02 1.94000e+01 2.10000e+01 2.37000e+01 6.11460e+00 3.29430e+00]\n",
      " [1.50000e+02 2.04000e+01 2.20000e+01 2.47000e+01 5.80450e+00 3.75440e+00]\n",
      " [1.45000e+02 2.05000e+01 2.20000e+01 2.43000e+01 6.63390e+00 3.54780e+00]\n",
      " [1.60000e+02 2.05000e+01 2.25000e+01 2.53000e+01 7.03340e+00 3.82030e+00]\n",
      " [1.40000e+02 2.10000e+01 2.25000e+01 2.50000e+01 6.55000e+00 3.32500e+00]\n",
      " [1.60000e+02 2.11000e+01 2.25000e+01 2.50000e+01 6.40000e+00 3.80000e+00]\n",
      " [1.69000e+02 2.20000e+01 2.40000e+01 2.72000e+01 7.53440e+00 3.83520e+00]\n",
      " [1.61000e+02 2.20000e+01 2.34000e+01 2.67000e+01 6.91530e+00 3.63120e+00]\n",
      " [2.00000e+02 2.21000e+01 2.35000e+01 2.68000e+01 7.39680e+00 4.12720e+00]\n",
      " [1.80000e+02 2.36000e+01 2.52000e+01 2.79000e+01 7.08660e+00 3.90600e+00]\n",
      " [2.90000e+02 2.40000e+01 2.60000e+01 2.92000e+01 8.87680e+00 4.49680e+00]\n",
      " [2.72000e+02 2.50000e+01 2.70000e+01 3.06000e+01 8.56800e+00 4.77360e+00]\n",
      " [3.90000e+02 2.95000e+01 3.17000e+01 3.50000e+01 9.48500e+00 5.35500e+00]\n",
      " [2.70000e+02 2.36000e+01 2.60000e+01 2.87000e+01 8.38040e+00 4.24760e+00]\n",
      " [2.70000e+02 2.41000e+01 2.65000e+01 2.93000e+01 8.14540e+00 4.24850e+00]\n",
      " [3.06000e+02 2.56000e+01 2.80000e+01 3.08000e+01 8.77800e+00 4.68160e+00]\n",
      " [5.40000e+02 2.85000e+01 3.10000e+01 3.40000e+01 1.07440e+01 6.56200e+00]\n",
      " [8.00000e+02 3.37000e+01 3.64000e+01 3.96000e+01 1.17612e+01 6.57360e+00]\n",
      " [1.00000e+03 3.73000e+01 4.00000e+01 4.35000e+01 1.23540e+01 6.52500e+00]\n",
      " [5.50000e+01 1.35000e+01 1.47000e+01 1.65000e+01 6.84750e+00 2.32650e+00]\n",
      " [6.00000e+01 1.43000e+01 1.55000e+01 1.74000e+01 6.57720e+00 2.31420e+00]\n",
      " [9.00000e+01 1.63000e+01 1.77000e+01 1.98000e+01 7.40520e+00 2.67300e+00]\n",
      " [1.20000e+02 1.75000e+01 1.90000e+01 2.13000e+01 8.39220e+00 2.91810e+00]\n",
      " [1.50000e+02 1.84000e+01 2.00000e+01 2.24000e+01 8.89280e+00 3.29280e+00]\n",
      " [1.40000e+02 1.90000e+01 2.07000e+01 2.32000e+01 8.53760e+00 3.29440e+00]\n",
      " [1.70000e+02 1.90000e+01 2.07000e+01 2.32000e+01 9.39600e+00 3.41040e+00]\n",
      " [1.45000e+02 1.98000e+01 2.15000e+01 2.41000e+01 9.73640e+00 3.15710e+00]\n",
      " [2.00000e+02 2.12000e+01 2.30000e+01 2.58000e+01 1.03458e+01 3.66360e+00]\n",
      " [2.73000e+02 2.30000e+01 2.50000e+01 2.80000e+01 1.10880e+01 4.14400e+00]\n",
      " [3.00000e+02 2.40000e+01 2.60000e+01 2.90000e+01 1.13680e+01 4.23400e+00]\n",
      " [5.90000e+00 7.50000e+00 8.40000e+00 8.80000e+00 2.11200e+00 1.40800e+00]\n",
      " [3.20000e+01 1.25000e+01 1.37000e+01 1.47000e+01 3.52800e+00 1.99920e+00]\n",
      " [4.00000e+01 1.38000e+01 1.50000e+01 1.60000e+01 3.82400e+00 2.43200e+00]\n",
      " [5.15000e+01 1.50000e+01 1.62000e+01 1.72000e+01 4.59240e+00 2.63160e+00]\n",
      " [7.00000e+01 1.57000e+01 1.74000e+01 1.85000e+01 4.58800e+00 2.94150e+00]\n",
      " [1.00000e+02 1.62000e+01 1.80000e+01 1.92000e+01 5.22240e+00 3.32160e+00]\n",
      " [7.80000e+01 1.68000e+01 1.87000e+01 1.94000e+01 5.19920e+00 3.12340e+00]\n",
      " [8.00000e+01 1.72000e+01 1.90000e+01 2.02000e+01 5.63580e+00 3.05020e+00]\n",
      " [8.50000e+01 1.78000e+01 1.96000e+01 2.08000e+01 5.13760e+00 3.03680e+00]\n",
      " [8.50000e+01 1.82000e+01 2.00000e+01 2.10000e+01 5.08200e+00 2.77200e+00]\n",
      " [1.10000e+02 1.90000e+01 2.10000e+01 2.25000e+01 5.69250e+00 3.55500e+00]\n",
      " [1.15000e+02 1.90000e+01 2.10000e+01 2.25000e+01 5.91750e+00 3.30750e+00]\n",
      " [1.25000e+02 1.90000e+01 2.10000e+01 2.25000e+01 5.69250e+00 3.66750e+00]\n",
      " [1.30000e+02 1.93000e+01 2.13000e+01 2.28000e+01 6.38400e+00 3.53400e+00]\n",
      " [1.20000e+02 2.00000e+01 2.20000e+01 2.35000e+01 6.11000e+00 3.40750e+00]\n",
      " [1.20000e+02 2.00000e+01 2.20000e+01 2.35000e+01 5.64000e+00 3.52500e+00]\n",
      " [1.30000e+02 2.00000e+01 2.20000e+01 2.35000e+01 6.11000e+00 3.52500e+00]\n",
      " [1.35000e+02 2.00000e+01 2.20000e+01 2.35000e+01 5.87500e+00 3.52500e+00]\n",
      " [1.10000e+02 2.00000e+01 2.20000e+01 2.35000e+01 5.52250e+00 3.99500e+00]\n",
      " [1.30000e+02 2.05000e+01 2.25000e+01 2.40000e+01 5.85600e+00 3.62400e+00]\n",
      " [1.50000e+02 2.05000e+01 2.25000e+01 2.40000e+01 6.79200e+00 3.62400e+00]\n",
      " [1.45000e+02 2.07000e+01 2.27000e+01 2.42000e+01 5.95320e+00 3.63000e+00]\n",
      " [1.50000e+02 2.10000e+01 2.30000e+01 2.45000e+01 5.21850e+00 3.62600e+00]\n",
      " [1.70000e+02 2.15000e+01 2.35000e+01 2.50000e+01 6.27500e+00 3.72500e+00]\n",
      " [2.25000e+02 2.20000e+01 2.40000e+01 2.55000e+01 7.29300e+00 3.72300e+00]\n",
      " [1.45000e+02 2.20000e+01 2.40000e+01 2.55000e+01 6.37500e+00 3.82500e+00]\n",
      " [1.88000e+02 2.26000e+01 2.46000e+01 2.62000e+01 6.73340e+00 4.16580e+00]\n",
      " [1.80000e+02 2.30000e+01 2.50000e+01 2.65000e+01 6.43950e+00 3.68350e+00]\n",
      " [1.97000e+02 2.35000e+01 2.56000e+01 2.70000e+01 6.56100e+00 4.23900e+00]\n",
      " [2.18000e+02 2.50000e+01 2.65000e+01 2.80000e+01 7.16800e+00 4.14400e+00]\n",
      " [3.00000e+02 2.52000e+01 2.73000e+01 2.87000e+01 8.32300e+00 5.13730e+00]\n",
      " [2.60000e+02 2.54000e+01 2.75000e+01 2.89000e+01 7.16720e+00 4.33500e+00]\n",
      " [2.65000e+02 2.54000e+01 2.75000e+01 2.89000e+01 7.05160e+00 4.33500e+00]\n",
      " [2.50000e+02 2.54000e+01 2.75000e+01 2.89000e+01 7.28280e+00 4.56620e+00]\n",
      " [2.50000e+02 2.59000e+01 2.80000e+01 2.94000e+01 7.82040e+00 4.20420e+00]\n",
      " [3.00000e+02 2.69000e+01 2.87000e+01 3.01000e+01 7.58520e+00 4.63540e+00]\n",
      " [3.20000e+02 2.78000e+01 3.00000e+01 3.16000e+01 7.61560e+00 4.77160e+00]\n",
      " [5.14000e+02 3.05000e+01 3.28000e+01 3.40000e+01 1.00300e+01 6.01800e+00]\n",
      " [5.56000e+02 3.20000e+01 3.45000e+01 3.65000e+01 1.02565e+01 6.38750e+00]\n",
      " [8.40000e+02 3.25000e+01 3.50000e+01 3.73000e+01 1.14884e+01 7.79570e+00]\n",
      " [6.85000e+02 3.40000e+01 3.65000e+01 3.90000e+01 1.08810e+01 6.86400e+00]\n",
      " [7.00000e+02 3.40000e+01 3.60000e+01 3.83000e+01 1.06091e+01 6.74080e+00]\n",
      " [7.00000e+02 3.45000e+01 3.70000e+01 3.94000e+01 1.08350e+01 6.26460e+00]\n",
      " [6.90000e+02 3.46000e+01 3.70000e+01 3.93000e+01 1.05717e+01 6.36660e+00]\n",
      " [9.00000e+02 3.65000e+01 3.90000e+01 4.14000e+01 1.11366e+01 7.49340e+00]\n",
      " [6.50000e+02 3.65000e+01 3.90000e+01 4.14000e+01 1.11366e+01 6.00300e+00]\n",
      " [8.20000e+02 3.66000e+01 3.90000e+01 4.13000e+01 1.24313e+01 7.35140e+00]\n",
      " [8.50000e+02 3.69000e+01 4.00000e+01 4.23000e+01 1.19286e+01 7.10640e+00]\n",
      " [9.00000e+02 3.70000e+01 4.00000e+01 4.25000e+01 1.17300e+01 7.22500e+00]\n",
      " [1.01500e+03 3.70000e+01 4.00000e+01 4.24000e+01 1.23808e+01 7.46240e+00]\n",
      " [8.20000e+02 3.71000e+01 4.00000e+01 4.25000e+01 1.11350e+01 6.63000e+00]\n",
      " [1.10000e+03 3.90000e+01 4.20000e+01 4.46000e+01 1.28002e+01 6.86840e+00]\n",
      " [1.00000e+03 3.98000e+01 4.30000e+01 4.52000e+01 1.19328e+01 7.27720e+00]\n",
      " [1.10000e+03 4.01000e+01 4.30000e+01 4.55000e+01 1.25125e+01 7.41650e+00]\n",
      " [1.00000e+03 4.02000e+01 4.35000e+01 4.60000e+01 1.26040e+01 8.14200e+00]\n",
      " [1.00000e+03 4.11000e+01 4.40000e+01 4.66000e+01 1.24888e+01 7.59580e+00]\n",
      " [2.00000e+02 3.00000e+01 3.23000e+01 3.48000e+01 5.56800e+00 3.37560e+00]\n",
      " [3.00000e+02 3.17000e+01 3.40000e+01 3.78000e+01 5.70780e+00 4.15800e+00]\n",
      " [3.00000e+02 3.27000e+01 3.50000e+01 3.88000e+01 5.93640e+00 4.38440e+00]\n",
      " [3.00000e+02 3.48000e+01 3.73000e+01 3.98000e+01 6.28840e+00 4.01980e+00]\n",
      " [4.30000e+02 3.55000e+01 3.80000e+01 4.05000e+01 7.29000e+00 4.57650e+00]\n",
      " [3.45000e+02 3.60000e+01 3.85000e+01 4.10000e+01 6.39600e+00 3.97700e+00]\n",
      " [4.56000e+02 4.00000e+01 4.25000e+01 4.55000e+01 7.28000e+00 4.32250e+00]\n",
      " [5.10000e+02 4.00000e+01 4.25000e+01 4.55000e+01 6.82500e+00 4.45900e+00]\n",
      " [5.40000e+02 4.01000e+01 4.30000e+01 4.58000e+01 7.78600e+00 5.12960e+00]\n",
      " [5.00000e+02 4.20000e+01 4.50000e+01 4.80000e+01 6.96000e+00 4.89600e+00]\n",
      " [5.67000e+02 4.32000e+01 4.60000e+01 4.87000e+01 7.79200e+00 4.87000e+00]\n",
      " [7.70000e+02 4.48000e+01 4.80000e+01 5.12000e+01 7.68000e+00 5.37600e+00]\n",
      " [9.50000e+02 4.83000e+01 5.17000e+01 5.51000e+01 8.92620e+00 6.17120e+00]\n",
      " [1.25000e+03 5.20000e+01 5.60000e+01 5.97000e+01 1.06863e+01 6.98490e+00]\n",
      " [1.60000e+03 5.60000e+01 6.00000e+01 6.40000e+01 9.60000e+00 6.14400e+00]\n",
      " [1.55000e+03 5.60000e+01 6.00000e+01 6.40000e+01 9.60000e+00 6.14400e+00]\n",
      " [1.65000e+03 5.90000e+01 6.34000e+01 6.80000e+01 1.08120e+01 7.48000e+00]\n",
      " [6.70000e+00 9.30000e+00 9.80000e+00 1.08000e+01 1.73880e+00 1.04760e+00]\n",
      " [7.50000e+00 1.00000e+01 1.05000e+01 1.16000e+01 1.97200e+00 1.16000e+00]\n",
      " [7.00000e+00 1.01000e+01 1.06000e+01 1.16000e+01 1.72840e+00 1.14840e+00]\n",
      " [9.70000e+00 1.04000e+01 1.10000e+01 1.20000e+01 2.19600e+00 1.38000e+00]\n",
      " [9.80000e+00 1.07000e+01 1.12000e+01 1.24000e+01 2.08320e+00 1.27720e+00]\n",
      " [8.70000e+00 1.08000e+01 1.13000e+01 1.26000e+01 1.97820e+00 1.28520e+00]\n",
      " [1.00000e+01 1.13000e+01 1.18000e+01 1.31000e+01 2.21390e+00 1.28380e+00]\n",
      " [9.90000e+00 1.13000e+01 1.18000e+01 1.31000e+01 2.21390e+00 1.16590e+00]\n",
      " [9.80000e+00 1.14000e+01 1.20000e+01 1.32000e+01 2.20440e+00 1.14840e+00]\n",
      " [1.22000e+01 1.15000e+01 1.22000e+01 1.34000e+01 2.09040e+00 1.39360e+00]\n",
      " [1.34000e+01 1.17000e+01 1.24000e+01 1.35000e+01 2.43000e+00 1.26900e+00]\n",
      " [1.22000e+01 1.21000e+01 1.30000e+01 1.38000e+01 2.27700e+00 1.25580e+00]\n",
      " [1.97000e+01 1.32000e+01 1.43000e+01 1.52000e+01 2.87280e+00 2.06720e+00]\n",
      " [1.99000e+01 1.38000e+01 1.50000e+01 1.62000e+01 2.93220e+00 1.87920e+00]]\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# in numpy everything needs to be of the same type to be put into an array\n",
    "raw_labels = np.genfromtxt(\"C:\\\\Users\\\\mccal\\\\Documents\\\\CCI\\\\Coding Five\\\\bsc-coding-five\\\\perceptron\\\\Fish.csv\", dtype=\"str\", delimiter=\",\", skip_header=1, usecols=(0))\n",
    "print(raw_labels)\n",
    "\n",
    "raw_data = np.genfromtxt(\"C:\\\\Users\\\\mccal\\\\Documents\\\\CCI\\\\Coding Five\\\\bsc-coding-five\\\\perceptron\\\\Fish.csv\", delimiter=\",\", skip_header=1, usecols=(1,2,3,4,5,6))\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f31344",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f89f194e",
   "metadata": {},
   "source": [
    "# Display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dfc00141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(x_train[i], cmap='gray')\n",
    "#     plt.title(y_train[i])\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea072f2",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "In this instance we keep the original shape of the data (ie. 28x28) as it can be flatten by Keras later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "74ae83ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.42000e+02 2.32000e+01 2.54000e+01 3.00000e+01 1.15200e+01 4.02000e+00]\n",
      " [2.90000e+02 2.40000e+01 2.63000e+01 3.12000e+01 1.24800e+01 4.30560e+00]\n",
      " [3.40000e+02 2.39000e+01 2.65000e+01 3.11000e+01 1.23778e+01 4.69610e+00]\n",
      " [3.63000e+02 2.63000e+01 2.90000e+01 3.35000e+01 1.27300e+01 4.45550e+00]\n",
      " [4.30000e+02 2.65000e+01 2.90000e+01 3.40000e+01 1.24440e+01 5.13400e+00]\n",
      " [4.50000e+02 2.68000e+01 2.97000e+01 3.47000e+01 1.36024e+01 4.92740e+00]\n",
      " [5.00000e+02 2.68000e+01 2.97000e+01 3.45000e+01 1.41795e+01 5.27850e+00]\n",
      " [3.90000e+02 2.76000e+01 3.00000e+01 3.50000e+01 1.26700e+01 4.69000e+00]\n",
      " [4.50000e+02 2.76000e+01 3.00000e+01 3.51000e+01 1.40049e+01 4.84380e+00]\n",
      " [5.00000e+02 2.85000e+01 3.07000e+01 3.62000e+01 1.42266e+01 4.95940e+00]\n",
      " [4.75000e+02 2.84000e+01 3.10000e+01 3.62000e+01 1.42628e+01 5.10420e+00]\n",
      " [5.00000e+02 2.87000e+01 3.10000e+01 3.62000e+01 1.43714e+01 4.81460e+00]\n",
      " [5.00000e+02 2.91000e+01 3.15000e+01 3.64000e+01 1.37592e+01 4.36800e+00]\n",
      " [3.40000e+02 2.95000e+01 3.20000e+01 3.73000e+01 1.39129e+01 5.07280e+00]\n",
      " [6.00000e+02 2.94000e+01 3.20000e+01 3.72000e+01 1.49544e+01 5.17080e+00]\n",
      " [6.00000e+02 2.94000e+01 3.20000e+01 3.72000e+01 1.54380e+01 5.58000e+00]\n",
      " [7.00000e+02 3.04000e+01 3.30000e+01 3.83000e+01 1.48604e+01 5.28540e+00]\n",
      " [7.00000e+02 3.04000e+01 3.30000e+01 3.85000e+01 1.49380e+01 5.19750e+00]\n",
      " [6.10000e+02 3.09000e+01 3.35000e+01 3.86000e+01 1.56330e+01 5.13380e+00]\n",
      " [6.50000e+02 3.10000e+01 3.35000e+01 3.87000e+01 1.44738e+01 5.72760e+00]\n",
      " [5.75000e+02 3.13000e+01 3.40000e+01 3.95000e+01 1.51285e+01 5.56950e+00]\n",
      " [6.85000e+02 3.14000e+01 3.40000e+01 3.92000e+01 1.59936e+01 5.37040e+00]\n",
      " [6.20000e+02 3.15000e+01 3.45000e+01 3.97000e+01 1.55227e+01 5.28010e+00]\n",
      " [6.80000e+02 3.18000e+01 3.50000e+01 4.06000e+01 1.54686e+01 6.13060e+00]\n",
      " [7.00000e+02 3.19000e+01 3.50000e+01 4.05000e+01 1.62405e+01 5.58900e+00]\n",
      " [7.25000e+02 3.18000e+01 3.50000e+01 4.09000e+01 1.63600e+01 6.05320e+00]\n",
      " [7.20000e+02 3.20000e+01 3.50000e+01 4.06000e+01 1.63618e+01 6.09000e+00]\n",
      " [7.14000e+02 3.27000e+01 3.60000e+01 4.15000e+01 1.65170e+01 5.85150e+00]\n",
      " [8.50000e+02 3.28000e+01 3.60000e+01 4.16000e+01 1.68896e+01 6.19840e+00]\n",
      " [1.00000e+03 3.35000e+01 3.70000e+01 4.26000e+01 1.89570e+01 6.60300e+00]\n",
      " [9.20000e+02 3.50000e+01 3.85000e+01 4.41000e+01 1.80369e+01 6.30630e+00]\n",
      " [9.55000e+02 3.50000e+01 3.85000e+01 4.40000e+01 1.80840e+01 6.29200e+00]\n",
      " [9.25000e+02 3.62000e+01 3.95000e+01 4.53000e+01 1.87542e+01 6.74970e+00]\n",
      " [9.75000e+02 3.74000e+01 4.10000e+01 4.59000e+01 1.86354e+01 6.74730e+00]\n",
      " [9.50000e+02 3.80000e+01 4.10000e+01 4.65000e+01 1.76235e+01 6.37050e+00]\n",
      " [4.00000e+01 1.29000e+01 1.41000e+01 1.62000e+01 4.14720e+00 2.26800e+00]\n",
      " [6.90000e+01 1.65000e+01 1.82000e+01 2.03000e+01 5.29830e+00 2.82170e+00]\n",
      " [7.80000e+01 1.75000e+01 1.88000e+01 2.12000e+01 5.57560e+00 2.90440e+00]\n",
      " [8.70000e+01 1.82000e+01 1.98000e+01 2.22000e+01 5.61660e+00 3.17460e+00]\n",
      " [1.20000e+02 1.86000e+01 2.00000e+01 2.22000e+01 6.21600e+00 3.57420e+00]\n",
      " [0.00000e+00 1.90000e+01 2.05000e+01 2.28000e+01 6.47520e+00 3.35160e+00]\n",
      " [1.10000e+02 1.91000e+01 2.08000e+01 2.31000e+01 6.16770e+00 3.39570e+00]\n",
      " [1.20000e+02 1.94000e+01 2.10000e+01 2.37000e+01 6.11460e+00 3.29430e+00]\n",
      " [1.50000e+02 2.04000e+01 2.20000e+01 2.47000e+01 5.80450e+00 3.75440e+00]\n",
      " [1.45000e+02 2.05000e+01 2.20000e+01 2.43000e+01 6.63390e+00 3.54780e+00]\n",
      " [1.60000e+02 2.05000e+01 2.25000e+01 2.53000e+01 7.03340e+00 3.82030e+00]\n",
      " [1.40000e+02 2.10000e+01 2.25000e+01 2.50000e+01 6.55000e+00 3.32500e+00]\n",
      " [1.60000e+02 2.11000e+01 2.25000e+01 2.50000e+01 6.40000e+00 3.80000e+00]\n",
      " [1.69000e+02 2.20000e+01 2.40000e+01 2.72000e+01 7.53440e+00 3.83520e+00]\n",
      " [1.61000e+02 2.20000e+01 2.34000e+01 2.67000e+01 6.91530e+00 3.63120e+00]\n",
      " [2.00000e+02 2.21000e+01 2.35000e+01 2.68000e+01 7.39680e+00 4.12720e+00]\n",
      " [1.80000e+02 2.36000e+01 2.52000e+01 2.79000e+01 7.08660e+00 3.90600e+00]\n",
      " [2.90000e+02 2.40000e+01 2.60000e+01 2.92000e+01 8.87680e+00 4.49680e+00]\n",
      " [2.72000e+02 2.50000e+01 2.70000e+01 3.06000e+01 8.56800e+00 4.77360e+00]\n",
      " [3.90000e+02 2.95000e+01 3.17000e+01 3.50000e+01 9.48500e+00 5.35500e+00]\n",
      " [2.70000e+02 2.36000e+01 2.60000e+01 2.87000e+01 8.38040e+00 4.24760e+00]\n",
      " [2.70000e+02 2.41000e+01 2.65000e+01 2.93000e+01 8.14540e+00 4.24850e+00]\n",
      " [3.06000e+02 2.56000e+01 2.80000e+01 3.08000e+01 8.77800e+00 4.68160e+00]\n",
      " [5.40000e+02 2.85000e+01 3.10000e+01 3.40000e+01 1.07440e+01 6.56200e+00]\n",
      " [8.00000e+02 3.37000e+01 3.64000e+01 3.96000e+01 1.17612e+01 6.57360e+00]\n",
      " [1.00000e+03 3.73000e+01 4.00000e+01 4.35000e+01 1.23540e+01 6.52500e+00]\n",
      " [5.50000e+01 1.35000e+01 1.47000e+01 1.65000e+01 6.84750e+00 2.32650e+00]\n",
      " [6.00000e+01 1.43000e+01 1.55000e+01 1.74000e+01 6.57720e+00 2.31420e+00]\n",
      " [9.00000e+01 1.63000e+01 1.77000e+01 1.98000e+01 7.40520e+00 2.67300e+00]\n",
      " [1.20000e+02 1.75000e+01 1.90000e+01 2.13000e+01 8.39220e+00 2.91810e+00]\n",
      " [1.50000e+02 1.84000e+01 2.00000e+01 2.24000e+01 8.89280e+00 3.29280e+00]\n",
      " [1.40000e+02 1.90000e+01 2.07000e+01 2.32000e+01 8.53760e+00 3.29440e+00]\n",
      " [1.70000e+02 1.90000e+01 2.07000e+01 2.32000e+01 9.39600e+00 3.41040e+00]\n",
      " [1.45000e+02 1.98000e+01 2.15000e+01 2.41000e+01 9.73640e+00 3.15710e+00]\n",
      " [2.00000e+02 2.12000e+01 2.30000e+01 2.58000e+01 1.03458e+01 3.66360e+00]\n",
      " [2.73000e+02 2.30000e+01 2.50000e+01 2.80000e+01 1.10880e+01 4.14400e+00]\n",
      " [3.00000e+02 2.40000e+01 2.60000e+01 2.90000e+01 1.13680e+01 4.23400e+00]\n",
      " [5.90000e+00 7.50000e+00 8.40000e+00 8.80000e+00 2.11200e+00 1.40800e+00]\n",
      " [3.20000e+01 1.25000e+01 1.37000e+01 1.47000e+01 3.52800e+00 1.99920e+00]\n",
      " [4.00000e+01 1.38000e+01 1.50000e+01 1.60000e+01 3.82400e+00 2.43200e+00]\n",
      " [5.15000e+01 1.50000e+01 1.62000e+01 1.72000e+01 4.59240e+00 2.63160e+00]\n",
      " [7.00000e+01 1.57000e+01 1.74000e+01 1.85000e+01 4.58800e+00 2.94150e+00]\n",
      " [1.00000e+02 1.62000e+01 1.80000e+01 1.92000e+01 5.22240e+00 3.32160e+00]\n",
      " [7.80000e+01 1.68000e+01 1.87000e+01 1.94000e+01 5.19920e+00 3.12340e+00]\n",
      " [8.00000e+01 1.72000e+01 1.90000e+01 2.02000e+01 5.63580e+00 3.05020e+00]\n",
      " [8.50000e+01 1.78000e+01 1.96000e+01 2.08000e+01 5.13760e+00 3.03680e+00]\n",
      " [8.50000e+01 1.82000e+01 2.00000e+01 2.10000e+01 5.08200e+00 2.77200e+00]\n",
      " [1.10000e+02 1.90000e+01 2.10000e+01 2.25000e+01 5.69250e+00 3.55500e+00]\n",
      " [1.15000e+02 1.90000e+01 2.10000e+01 2.25000e+01 5.91750e+00 3.30750e+00]\n",
      " [1.25000e+02 1.90000e+01 2.10000e+01 2.25000e+01 5.69250e+00 3.66750e+00]\n",
      " [1.30000e+02 1.93000e+01 2.13000e+01 2.28000e+01 6.38400e+00 3.53400e+00]\n",
      " [1.20000e+02 2.00000e+01 2.20000e+01 2.35000e+01 6.11000e+00 3.40750e+00]\n",
      " [1.20000e+02 2.00000e+01 2.20000e+01 2.35000e+01 5.64000e+00 3.52500e+00]\n",
      " [1.30000e+02 2.00000e+01 2.20000e+01 2.35000e+01 6.11000e+00 3.52500e+00]\n",
      " [1.35000e+02 2.00000e+01 2.20000e+01 2.35000e+01 5.87500e+00 3.52500e+00]\n",
      " [1.10000e+02 2.00000e+01 2.20000e+01 2.35000e+01 5.52250e+00 3.99500e+00]\n",
      " [1.30000e+02 2.05000e+01 2.25000e+01 2.40000e+01 5.85600e+00 3.62400e+00]\n",
      " [1.50000e+02 2.05000e+01 2.25000e+01 2.40000e+01 6.79200e+00 3.62400e+00]\n",
      " [1.45000e+02 2.07000e+01 2.27000e+01 2.42000e+01 5.95320e+00 3.63000e+00]\n",
      " [1.50000e+02 2.10000e+01 2.30000e+01 2.45000e+01 5.21850e+00 3.62600e+00]\n",
      " [1.70000e+02 2.15000e+01 2.35000e+01 2.50000e+01 6.27500e+00 3.72500e+00]\n",
      " [2.25000e+02 2.20000e+01 2.40000e+01 2.55000e+01 7.29300e+00 3.72300e+00]\n",
      " [1.45000e+02 2.20000e+01 2.40000e+01 2.55000e+01 6.37500e+00 3.82500e+00]\n",
      " [1.88000e+02 2.26000e+01 2.46000e+01 2.62000e+01 6.73340e+00 4.16580e+00]\n",
      " [1.80000e+02 2.30000e+01 2.50000e+01 2.65000e+01 6.43950e+00 3.68350e+00]\n",
      " [1.97000e+02 2.35000e+01 2.56000e+01 2.70000e+01 6.56100e+00 4.23900e+00]\n",
      " [2.18000e+02 2.50000e+01 2.65000e+01 2.80000e+01 7.16800e+00 4.14400e+00]\n",
      " [3.00000e+02 2.52000e+01 2.73000e+01 2.87000e+01 8.32300e+00 5.13730e+00]\n",
      " [2.60000e+02 2.54000e+01 2.75000e+01 2.89000e+01 7.16720e+00 4.33500e+00]\n",
      " [2.65000e+02 2.54000e+01 2.75000e+01 2.89000e+01 7.05160e+00 4.33500e+00]\n",
      " [2.50000e+02 2.54000e+01 2.75000e+01 2.89000e+01 7.28280e+00 4.56620e+00]\n",
      " [2.50000e+02 2.59000e+01 2.80000e+01 2.94000e+01 7.82040e+00 4.20420e+00]\n",
      " [3.00000e+02 2.69000e+01 2.87000e+01 3.01000e+01 7.58520e+00 4.63540e+00]\n",
      " [3.20000e+02 2.78000e+01 3.00000e+01 3.16000e+01 7.61560e+00 4.77160e+00]\n",
      " [5.14000e+02 3.05000e+01 3.28000e+01 3.40000e+01 1.00300e+01 6.01800e+00]\n",
      " [5.56000e+02 3.20000e+01 3.45000e+01 3.65000e+01 1.02565e+01 6.38750e+00]\n",
      " [8.40000e+02 3.25000e+01 3.50000e+01 3.73000e+01 1.14884e+01 7.79570e+00]\n",
      " [6.85000e+02 3.40000e+01 3.65000e+01 3.90000e+01 1.08810e+01 6.86400e+00]\n",
      " [7.00000e+02 3.40000e+01 3.60000e+01 3.83000e+01 1.06091e+01 6.74080e+00]\n",
      " [7.00000e+02 3.45000e+01 3.70000e+01 3.94000e+01 1.08350e+01 6.26460e+00]\n",
      " [6.90000e+02 3.46000e+01 3.70000e+01 3.93000e+01 1.05717e+01 6.36660e+00]\n",
      " [9.00000e+02 3.65000e+01 3.90000e+01 4.14000e+01 1.11366e+01 7.49340e+00]\n",
      " [6.50000e+02 3.65000e+01 3.90000e+01 4.14000e+01 1.11366e+01 6.00300e+00]\n",
      " [8.20000e+02 3.66000e+01 3.90000e+01 4.13000e+01 1.24313e+01 7.35140e+00]\n",
      " [8.50000e+02 3.69000e+01 4.00000e+01 4.23000e+01 1.19286e+01 7.10640e+00]\n",
      " [9.00000e+02 3.70000e+01 4.00000e+01 4.25000e+01 1.17300e+01 7.22500e+00]\n",
      " [1.01500e+03 3.70000e+01 4.00000e+01 4.24000e+01 1.23808e+01 7.46240e+00]\n",
      " [8.20000e+02 3.71000e+01 4.00000e+01 4.25000e+01 1.11350e+01 6.63000e+00]\n",
      " [1.10000e+03 3.90000e+01 4.20000e+01 4.46000e+01 1.28002e+01 6.86840e+00]\n",
      " [1.00000e+03 3.98000e+01 4.30000e+01 4.52000e+01 1.19328e+01 7.27720e+00]\n",
      " [1.10000e+03 4.01000e+01 4.30000e+01 4.55000e+01 1.25125e+01 7.41650e+00]\n",
      " [1.00000e+03 4.02000e+01 4.35000e+01 4.60000e+01 1.26040e+01 8.14200e+00]\n",
      " [1.00000e+03 4.11000e+01 4.40000e+01 4.66000e+01 1.24888e+01 7.59580e+00]\n",
      " [2.00000e+02 3.00000e+01 3.23000e+01 3.48000e+01 5.56800e+00 3.37560e+00]\n",
      " [3.00000e+02 3.17000e+01 3.40000e+01 3.78000e+01 5.70780e+00 4.15800e+00]\n",
      " [3.00000e+02 3.27000e+01 3.50000e+01 3.88000e+01 5.93640e+00 4.38440e+00]\n",
      " [3.00000e+02 3.48000e+01 3.73000e+01 3.98000e+01 6.28840e+00 4.01980e+00]\n",
      " [4.30000e+02 3.55000e+01 3.80000e+01 4.05000e+01 7.29000e+00 4.57650e+00]\n",
      " [3.45000e+02 3.60000e+01 3.85000e+01 4.10000e+01 6.39600e+00 3.97700e+00]\n",
      " [4.56000e+02 4.00000e+01 4.25000e+01 4.55000e+01 7.28000e+00 4.32250e+00]\n",
      " [5.10000e+02 4.00000e+01 4.25000e+01 4.55000e+01 6.82500e+00 4.45900e+00]\n",
      " [5.40000e+02 4.01000e+01 4.30000e+01 4.58000e+01 7.78600e+00 5.12960e+00]\n",
      " [5.00000e+02 4.20000e+01 4.50000e+01 4.80000e+01 6.96000e+00 4.89600e+00]\n",
      " [5.67000e+02 4.32000e+01 4.60000e+01 4.87000e+01 7.79200e+00 4.87000e+00]\n",
      " [7.70000e+02 4.48000e+01 4.80000e+01 5.12000e+01 7.68000e+00 5.37600e+00]\n",
      " [9.50000e+02 4.83000e+01 5.17000e+01 5.51000e+01 8.92620e+00 6.17120e+00]\n",
      " [1.25000e+03 5.20000e+01 5.60000e+01 5.97000e+01 1.06863e+01 6.98490e+00]\n",
      " [1.60000e+03 5.60000e+01 6.00000e+01 6.40000e+01 9.60000e+00 6.14400e+00]\n",
      " [1.55000e+03 5.60000e+01 6.00000e+01 6.40000e+01 9.60000e+00 6.14400e+00]\n",
      " [1.65000e+03 5.90000e+01 6.34000e+01 6.80000e+01 1.08120e+01 7.48000e+00]\n",
      " [6.70000e+00 9.30000e+00 9.80000e+00 1.08000e+01 1.73880e+00 1.04760e+00]\n",
      " [7.50000e+00 1.00000e+01 1.05000e+01 1.16000e+01 1.97200e+00 1.16000e+00]\n",
      " [7.00000e+00 1.01000e+01 1.06000e+01 1.16000e+01 1.72840e+00 1.14840e+00]\n",
      " [9.70000e+00 1.04000e+01 1.10000e+01 1.20000e+01 2.19600e+00 1.38000e+00]\n",
      " [9.80000e+00 1.07000e+01 1.12000e+01 1.24000e+01 2.08320e+00 1.27720e+00]\n",
      " [8.70000e+00 1.08000e+01 1.13000e+01 1.26000e+01 1.97820e+00 1.28520e+00]\n",
      " [1.00000e+01 1.13000e+01 1.18000e+01 1.31000e+01 2.21390e+00 1.28380e+00]\n",
      " [9.90000e+00 1.13000e+01 1.18000e+01 1.31000e+01 2.21390e+00 1.16590e+00]\n",
      " [9.80000e+00 1.14000e+01 1.20000e+01 1.32000e+01 2.20440e+00 1.14840e+00]\n",
      " [1.22000e+01 1.15000e+01 1.22000e+01 1.34000e+01 2.09040e+00 1.39360e+00]\n",
      " [1.34000e+01 1.17000e+01 1.24000e+01 1.35000e+01 2.43000e+00 1.26900e+00]\n",
      " [1.22000e+01 1.21000e+01 1.30000e+01 1.38000e+01 2.27700e+00 1.25580e+00]\n",
      " [1.97000e+01 1.32000e+01 1.43000e+01 1.52000e+01 2.87280e+00 2.06720e+00]\n",
      " [1.99000e+01 1.38000e+01 1.50000e+01 1.62000e+01 2.93220e+00 1.87920e+00]]\n",
      "['Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream'\n",
      " 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream'\n",
      " 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream'\n",
      " 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Bream' 'Roach'\n",
      " 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach'\n",
      " 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach' 'Roach'\n",
      " 'Roach' 'Whitefish' 'Whitefish' 'Whitefish' 'Whitefish' 'Whitefish'\n",
      " 'Whitefish' 'Parkki' 'Parkki' 'Parkki' 'Parkki' 'Parkki' 'Parkki'\n",
      " 'Parkki' 'Parkki' 'Parkki' 'Parkki' 'Parkki' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch'\n",
      " 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Perch' 'Pike'\n",
      " 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike'\n",
      " 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Pike' 'Smelt' 'Smelt' 'Smelt' 'Smelt'\n",
      " 'Smelt' 'Smelt' 'Smelt' 'Smelt' 'Smelt' 'Smelt' 'Smelt' 'Smelt' 'Smelt'\n",
      " 'Smelt']\n"
     ]
    }
   ],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "# x_train = x_train.astype(\"float32\") / 255\n",
    "# x_test = x_test.astype(\"float32\") / 255\n",
    "# # Make sure images have shape (28, 28, 1)\n",
    "# x_train = np.expand_dims(x_train, -1)\n",
    "# x_test = np.expand_dims(x_test, -1)\n",
    "# print(\"x_train shape:\", x_train.shape)\n",
    "# print(x_train.shape[0], \"train samples\")\n",
    "# print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# Model / data parameters\n",
    "# num_classes = 10\n",
    "# input_shape = x_train.shape[1:]\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "# y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "x_train = raw_data\n",
    "y_train = raw_labels\n",
    "# to_categorical need integers to convert a class vector into a binary class matrix\n",
    "# y_train = keras.utils.to_categorical(raw_labels, 7, dtype=\"int\")\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5c317",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "\n",
    "We have the usual input and output layer plus just after the input layer we have a flatten layer to make sure the data passed to our network is 1D. Next we have three fully connected hidden layers of various size but all uses the ReLU activation function. The final activation of the output layer is the Softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "12d00274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_42 (Dense)            (None, 10)                70        \n",
      "                                                                 \n",
      " dense_43 (Dense)            (None, 7)                 77        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 147 (588.00 Byte)\n",
      "Trainable params: 147 (588.00 Byte)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab = [\"Bream\", \"Roach\", \"Whitefish\", \"Parkki\", \"Perch\", \"Pike\", \"Smelt\"]\n",
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape=(6)),\n",
    "        # layers.Flatten(),\n",
    "        layers.Dense(10, activation=\"relu\"),\n",
    "        # layers.Dense(64, activation=\"relu\"),\n",
    "        # layers.Dense(48, activation=\"relu\"),\n",
    "        layers.Dense(7, activation=\"sigmoid\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12754e9",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "We train the model with a batch size of 128, ie 128 samples were passed through the network and the errors averaged before we adjusted the weights, then the next 128 samples were passed through. The network is trained for a total of 15 epochs (15 total passes through the whole training dataset).\n",
    "\n",
    "The loss function in use here is categorical cross-entropy which is a common loss function used for classification problems. We are also using the ADAM optimizer here to control the overall learning rate without defining a constant learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "e9276d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whitefish\n",
      "     Bream  Parkki  Perch   Pike  Roach  Smelt  Whitefish\n",
      "0     True   False  False  False  False  False      False\n",
      "1     True   False  False  False  False  False      False\n",
      "2     True   False  False  False  False  False      False\n",
      "3     True   False  False  False  False  False      False\n",
      "4     True   False  False  False  False  False      False\n",
      "..     ...     ...    ...    ...    ...    ...        ...\n",
      "154  False   False  False  False  False   True      False\n",
      "155  False   False  False  False  False   True      False\n",
      "156  False   False  False  False  False   True      False\n",
      "157  False   False  False  False  False   True      False\n",
      "158  False   False  False  False  False   True      False\n",
      "\n",
      "[159 rows x 7 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'pandas.core.frame.DataFrame'>]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[77], line 27\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train)\n\u001b[0;32m     25\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m\"\u001b[39m, optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 27\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\engine\\data_adapter.py:1776\u001b[0m, in \u001b[0;36mtrain_validation_split\u001b[1;34m(arrays, validation_split)\u001b[0m\n\u001b[0;32m   1774\u001b[0m unsplitable \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mtype\u001b[39m(t) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m flat_arrays \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _can_split(t)]\n\u001b[0;32m   1775\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m unsplitable:\n\u001b[1;32m-> 1776\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1777\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`validation_split` is only supported for Tensors or NumPy \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1778\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marrays, found following types in the input: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(unsplitable)\n\u001b[0;32m   1779\u001b[0m     )\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(t \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m flat_arrays):\n\u001b[0;32m   1782\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arrays, arrays\n",
      "\u001b[1;31mValueError\u001b[0m: `validation_split` is only supported for Tensors or NumPy arrays, found following types in the input: [<class 'pandas.core.frame.DataFrame'>]"
     ]
    }
   ],
   "source": [
    "# means after x pieces of data are forward passed it will average the errors, then do a backpropogation...it goes faster, and any odd data will be averaged out\n",
    "batch_size = 1 # we're only using 1 because our dataset is very small\n",
    "epochs = 15\n",
    "\n",
    "# ! accuracy was never above 50%\n",
    "# def one_hot(array):\n",
    "#     # unique makes sure repeated values are treated as unique separate values\n",
    "#     # inverse gives you the opposite\n",
    "#     unique, inverse = np.unique(array, return_inverse=True)\n",
    "#     # np.eye gives you an eye array that looks something like this [0, 0, 1, 0, 0]\n",
    "#     onehot = np.eye(unique.shape[0])[inverse]\n",
    "#     return onehot\n",
    "\n",
    "# ! in jupyter notebook if you assign a variable back into its self be careful of when you run cells...values can get muddled up    \n",
    "# y_train = one_hot(y_train)\n",
    "# print(y_train.shape)\n",
    "# print(y_train)\n",
    "\n",
    "# ! accuracy was always around 50%....too little data? <- tried decreasing the num of hidden layers and accuracy increased to 60%\n",
    "y_train = pd.Series(raw_labels)\n",
    "print(y_train.loc[58])\n",
    "y_train = pd.get_dummies(y_train)\n",
    "print(y_train)\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c621ee",
   "metadata": {},
   "source": [
    "# Evaluate the trained model\n",
    "\n",
    "We expect a test accuracy of about 97% here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422501d-f238-4071-9320-5228b06b1dc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
