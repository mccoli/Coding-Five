{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e669f36f",
   "metadata": {},
   "source": [
    "*Adapted from: https://keras.io/examples/vision/mnist_convnet/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b60e0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ee9c9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras import layers\n",
    "print(keras.__version__)\n",
    "keras.backend.backend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4223bf8",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d439f4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data, split between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f194e",
   "metadata": {},
   "source": [
    "# Display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc00141",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(x_train[i], cmap='gray')\n",
    "    plt.title(y_train[i])\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea072f2",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "In this instance we keep the original shape of the data (ie. 28x28) as it can be flatten by Keras later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ae83ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = x_train.shape[1:]\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5c317",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "\n",
    "We have the usual input and output layer plus just after the input layer we have a flatten layer to make sure the data passed to our network is 1D. Next we have three fully connected hidden layers of various size but all uses the ReLU activation function. The final activation of the output layer is the Softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d00274",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape=input_shape),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(48, activation=\"relu\"),\n",
    "        layers.Dense(64, activation=\"relu\"),\n",
    "        layers.Dense(48, activation=\"relu\"),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12754e9",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "We train the model with a batch size of 128, ie 128 samples were passed through the network and the errors averaged before we adjusted the weights, then the next 128 samples were passed through. The network is trained for a total of 15 epochs (15 total passes through the whole training dataset).\n",
    "\n",
    "The loss function in use here is categorical cross-entropy which is a common loss function used for classification problems. We are also using the ADAM optimizer here to control the overall learning rate without defining a constant learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9276d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c621ee",
   "metadata": {},
   "source": [
    "# Evaluate the trained model\n",
    "\n",
    "We expect a test accuracy of about 97% here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2b4f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
