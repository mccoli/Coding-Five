{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e669f36f",
   "metadata": {},
   "source": [
    "*Adapted from: https://keras.io/examples/vision/mnist_convnet/*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "646b60e0",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6ee9c9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mccal\\miniconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tensorflow'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# in keras you can define layers\n",
    "from tensorflow.keras import layers\n",
    "# print(keras.__version__)\n",
    "keras.backend.backend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "79ff124d-6638-47bf-8c38-a67f6271d6fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4223bf8",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d439f4ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[         nan          nan  2.21900e+05 ... -1.22257e+02  1.34000e+03\n",
      "   5.65000e+03]\n",
      " [         nan          nan  5.38000e+05 ... -1.22319e+02  1.69000e+03\n",
      "   7.63900e+03]\n",
      " [         nan          nan  1.80000e+05 ... -1.22233e+02  2.72000e+03\n",
      "   8.06200e+03]\n",
      " ...\n",
      " [         nan          nan  4.02101e+05 ... -1.22299e+02  1.02000e+03\n",
      "   2.00700e+03]\n",
      " [         nan          nan  4.00000e+05 ... -1.22069e+02  1.41000e+03\n",
      "   1.28700e+03]\n",
      " [         nan          nan  3.25000e+05 ... -1.22299e+02  1.02000e+03\n",
      "   1.35700e+03]]\n"
     ]
    }
   ],
   "source": [
    "# the data, split between train and test sets\n",
    "# (x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "raw_data = np.genfromtxt(\"kc_house_data.csv\", delimiter=\",\", skip_header=1)\n",
    "print(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89f194e",
   "metadata": {},
   "source": [
    "# Display the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dfc00141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.clf()\n",
    "# for i in range(9):\n",
    "#     ax = plt.subplot(3, 3, i + 1)\n",
    "#     plt.imshow(x_train[i], cmap='gray')\n",
    "#     plt.title(y_train[i])\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea072f2",
   "metadata": {},
   "source": [
    "# Prepare the data\n",
    "\n",
    "In this instance we keep the original shape of the data (ie. 28x28) as it can be flatten by Keras later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74ae83ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.27272727 0.00342142]\n",
      " [0.27272727 0.00438548]\n",
      " [0.18181818 0.00605562]\n",
      " ...\n",
      " [0.18181818 0.00081751]\n",
      " [0.27272727 0.00144608]\n",
      " [0.18181818 0.00065158]]\n",
      "[0.02881818 0.06987013 0.02337662 ... 0.05222091 0.05194805 0.04220779]\n",
      "11.0\n"
     ]
    }
   ],
   "source": [
    "x_max_1 = np.max(raw_data[:, 3])\n",
    "x_max_2 = np.max(raw_data[:, 6])\n",
    "x_train = raw_data[:, [3, 6] ] / [x_max_1, x_max_2]\n",
    "\n",
    "y_max = np.max(raw_data[:, 2])\n",
    "y_train = raw_data[:, 2] / y_max\n",
    "\n",
    "print(x_train)\n",
    "print(y_train)\n",
    "\n",
    "print(x_max_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e5c317",
   "metadata": {},
   "source": [
    "# Build the model\n",
    "\n",
    "We have the usual input and output layer plus just after the input layer we have a flatten layer to make sure the data passed to our network is 1D. Next we have three fully connected hidden layers of various size but all uses the ReLU activation function. The final activation of the output layer is the Softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12d00274",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mccal\\miniconda3\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 50)                150       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 20)                1020      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1191 (4.65 KB)\n",
      "Trainable params: 1191 (4.65 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.InputLayer(input_shape=(2)),\n",
    "        layers.Dense(50, activation=\"relu\"),\n",
    "        layers.Dense(20, activation=\"relu\"),\n",
    "        layers.Dense(1, activation=\"relu\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12754e9",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "We train the model with a batch size of 128, ie 128 samples were passed through the network and the errors averaged before we adjusted the weights, then the next 128 samples were passed through. The network is trained for a total of 15 epochs (15 total passes through the whole training dataset).\n",
    "\n",
    "The loss function in use here is categorical cross-entropy which is a common loss function used for classification problems. We are also using the ADAM optimizer here to control the overall learning rate without defining a constant learning rate.\n",
    "\n",
    "why no metrics? what is mean squared error loss? why is thi better for a regression problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9276d79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\mccal\\miniconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Epoch 1/15\n",
      "WARNING:tensorflow:From C:\\Users\\mccal\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "973/973 [==============================] - 7s 5ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 2/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 3/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 4/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 5/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 6/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 7/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 8/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 9/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 10/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 11/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 12/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 13/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 14/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n",
      "Epoch 15/15\n",
      "973/973 [==============================] - 4s 4ms/step - loss: 0.0071 - val_loss: 0.0082\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x2981a099c90>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# means after x pieces of data are forward passed it will average the errors, then do a backpropogation...it goes faster, and any odd data will be averaged out\n",
    "batch_size = 20\n",
    "epochs = 15\n",
    "\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=\"adam\")\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
